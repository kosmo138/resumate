{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keyword\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# from app.service.keyword_bak import insert_keyword_results\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# from selenium.webdriver.chrome.options import Options as ChromeOptions\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'app.core'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from konlpy.tag import Kkma\n",
    "from gensim.models import Word2Vec\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "from app.core.models import Keyword\n",
    "from app.core.database import session\n",
    "# from app.service.keyword_bak import insert_keyword_results\n",
    "# from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "\n",
    "class GoogleSearchScraper:\n",
    "\n",
    "    \n",
    "    def __init__(self, chrome_options):\n",
    "        self.chrome_options = chrome_options\n",
    "        self.driver = None\n",
    "   \n",
    "\n",
    "    def scroll_down_page(self):\n",
    "        SCROLL_PAUSE_TIME = 2\n",
    "\n",
    "        # Get scroll height\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            #새로운 스크롤 높이를 last_height로 설정하여 계속해서 스크롤을 진행\n",
    "\n",
    "    def get_first_search_result_url(self, query):\n",
    "        try:\n",
    "            print(\"크롬 웹 드라이버를 실행합니다...\")\n",
    "            self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "            print(\"구글 검색 페이지에 접근합니다...\")\n",
    "            self.driver.get(f\"https://www.google.com/search?q={query}\" + \" 인재상\")\n",
    "            # self.scroll_down_page()  # 스크롤을 내립니다.\n",
    "            print(\"첫 번째 항목의 URL을 가져오는 중...\")\n",
    "            # 일정 시간 동안 기다립니다.\n",
    "            first_title = WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"h3.LC20lb.MBeuO.DKV0Md\")\n",
    "                )\n",
    "            )\n",
    "            url = first_title.find_element(By.XPATH, \"./parent::a\").get_attribute(\n",
    "                \"href\"\n",
    "            )\n",
    "            print(\"첫 번째 검색 결과 URL을 성공적으로 가져왔습니다.\")\n",
    "            print(\"첫 번째 검색 결과 URL:\", url)\n",
    "            return url\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout occurred while waiting for element to load.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"검색 결과 URL을 가져오는 중 오류가 발생했습니다: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n",
    "\n",
    "# Word2Vec 모델 로드 및 관련 키워드 추출\n",
    "class KeywordExtractor:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.model = self.load_word2vec_model()\n",
    "\n",
    "    def load_word2vec_model(self):\n",
    "        try:\n",
    "            print(\"Word2Vec 모델을 로드하는 중...\")\n",
    "            model = Word2Vec.load(self.model_path)\n",
    "            print(\"Word2Vec 모델을 성공적으로 로드했습니다.\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred while loading Word2Vec model:\", e)\n",
    "            return None\n",
    "\n",
    "    def extract_related_keywords(self, nouns, target_words, threshold=0.6):\n",
    "        related_keywords = set()\n",
    "        for target_word in target_words:\n",
    "            for noun in nouns:\n",
    "                try:\n",
    "                    similarity = self.model.wv.similarity(noun, target_word)\n",
    "                    if similarity >= threshold:\n",
    "                        related_keywords.add(noun)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        return related_keywords\n",
    "\n",
    "\n",
    "class TextAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "\n",
    "    def extract_nouns_from_html(self, url):\n",
    "        try:\n",
    "            print(\"URL에서 HTML 내용을 가져옵니다...\")\n",
    "            response = requests.get(url, verify=False)\n",
    "            html_text = response.text\n",
    "            print(\"HTML 내용을 성공적으로 가져왔습니다.\")\n",
    "            print(\"HTML을 파싱합니다...\")\n",
    "            soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "            text = soup.get_text()\n",
    "            print(\"HTML을 성공적으로 파싱했습니다.\")\n",
    "            print(\"명사를 추출합니다...\")\n",
    "            nouns = self.kkma.nouns(text)\n",
    "            print(\"명사를 성공적으로 추출했습니다.\")\n",
    "            return nouns\n",
    "        except Exception as e:\n",
    "            print(f\"명사를 추출하는 데 실패했습니다: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def extract_keywords(csv_file_path):\n",
    "    # Word2Vec 모델 경로\n",
    "    model_path = \"wiki.model\"\n",
    "    # 목표 단어 리스트\n",
    "    target_words = [\n",
    "        \"인재\",\n",
    "        \"소통\",\n",
    "        \"능력\",\n",
    "        \"책임\",\n",
    "        \"성실\",\n",
    "        \"도전\",\n",
    "        \"열정\",\n",
    "        \"꿈\",\n",
    "        \"사람\",\n",
    "        \"다양\",\n",
    "        \"창의\",\n",
    "        \"혁신\",\n",
    "        \"전문\",\n",
    "        \"기술\",\n",
    "        \"지식\",\n",
    "        \"해결\",\n",
    "        \"리더십\",\n",
    "        \"팀워크\",\n",
    "        \"목표\",\n",
    "        \"정직\",\n",
    "        \"도덕\",\n",
    "        \"학습\",\n",
    "        \"성장\",\n",
    "        \"비전\",\n",
    "    ]\n",
    "\n",
    "    # GoogleSearchScraper, KeywordExtractor, TextAnalyzer 객체 생성\n",
    "    options = Options()\n",
    "    options.add_argument(\n",
    "            \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\"\n",
    "        )\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    scraper = GoogleSearchScraper(options)\n",
    "    extractor = KeywordExtractor(model_path)\n",
    "    analyzer = TextAnalyzer()\n",
    "\n",
    "    extracted_data = []\n",
    "\n",
    "    try:\n",
    "        with open(csv_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            for row in csv_reader:\n",
    "                if row:  # 빈 행이 아닌 경우에만 회사 이름 데이터를 가져옴\n",
    "                    company_name = row[0]\n",
    "                    print(f\"Processing company: {company_name}\")\n",
    "\n",
    "                    # 회사 이름을 사용하여 검색 결과 URL 가져오기\n",
    "                    url = scraper.get_first_search_result_url(company_name)\n",
    "                    if url:\n",
    "                        # URL에서 HTML 내용을 추출하여 명사 추출\n",
    "                        nouns = analyzer.extract_nouns_from_html(url)\n",
    "                        if nouns:\n",
    "                            # 추출된 명사를 사용하여 관련 키워드 추출\n",
    "                            related_keywords = extractor.extract_related_keywords(\n",
    "                                nouns, target_words\n",
    "                            )\n",
    "                            if related_keywords:\n",
    "                                print(\n",
    "                                    f\"Related keywords for {company_name}: {related_keywords}\"\n",
    "                                )\n",
    "                                # 추출된 데이터를 딕셔너리 형태로 저장\n",
    "                                company_data = {\n",
    "                                    \"company\": company_name,\n",
    "                                    \"keywords\": list(related_keywords),\n",
    "                                }\n",
    "                                extracted_data.append(company_data)\n",
    "                            else:\n",
    "                                print(f\"No related keywords found for {company_name}\")\n",
    "                        else:\n",
    "                            print(f\"No nouns extracted from HTML for {company_name}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to get search result URL for {company_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "def insert_keyword(json_keywords):\n",
    "    # JSON 데이터를 파이썬 객체로 변환\n",
    "    extracted_data = json.loads(json_keywords)\n",
    "\n",
    "    # 반복문을 통해 각 키워드 데이터를 데이터베이스에 추가\n",
    "    for data in extracted_data:\n",
    "        company = data.get(\"company\")\n",
    "        keywords = data.get(\"keywords\")\n",
    "\n",
    "        # Keyword 모델을 사용하여 새로운 키워드 객체 생성\n",
    "        new_keyword = Keyword(company=company, keyword=json.dumps(keywords))\n",
    "\n",
    "        # 새로운 키워드를 세션에 추가\n",
    "        session.add(new_keyword)\n",
    "\n",
    "    # 세션에 추가된 모든 변경 사항을 데이터베이스에 커밋하여 저장\n",
    "    session.commit()\n",
    "\n",
    "\n",
    "\n",
    "def main(csv_file_path):\n",
    "    try:\n",
    "        print(\"Extracting keywords from companies listed in the CSV file...\")\n",
    "        extracted_data = extract_keywords(csv_file_path)\n",
    "        print(\"Keywords extraction completed.\")\n",
    "        # 추출된 데이터를 JSON 형식으로 출력\n",
    "        # insert_keyword_results(extracted_data)\n",
    "        json_keywords = json.dumps(extracted_data, indent=4, ensure_ascii=False)\n",
    "        insert_keyword(json_keywords)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during keyword extraction: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = \"companies.csv\"  # CSV 파일 경로\n",
    "    main(csv_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
